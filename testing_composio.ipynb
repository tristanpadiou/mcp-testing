{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    ")\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "load_dotenv()\n",
    "google_api_key=os.getenv('google_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key=os.getenv('google_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL='gemini-2.0-flash'\n",
    "llm = ChatGoogleGenerativeAI(google_api_key=google_api_key, model=GEMINI_MODEL, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[List, add_messages]\n",
    "\n",
    "from composio_langgraph import Action, ComposioToolSet, App\n",
    "composio_toolset = ComposioToolSet(api_key=\"ui9jbd366fmtu1y6ekotd\")\n",
    "tools = composio_toolset.get_tools(apps=[App.GOOGLECALENDAR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)\n",
    "graph_builder = StateGraph(State)\n",
    "tool_node = ToolNode(tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\" travel assistant that answers user questions about their trip.\n",
    "    Depending on the request, leverage which tools to use if necessary.\"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "        \n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "memory=MemorySaver()\n",
    "graph=graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(input:str):\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    response=graph.invoke({'messages':HumanMessage(content=str(input))},config)\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat('list events starting with today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerStreamableHTTP\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.messages import ModelMessage\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Message_state:\n",
    "    messages:list[ModelMessage]\n",
    "\n",
    "class Agent_MCP:\n",
    "    def __init__(self, mcp_server_url:str, api_keys:dict):\n",
    "        self.mcp_server_url=mcp_server_url\n",
    "        self.api_keys=api_keys\n",
    "        self.tools=ComposioToolSet(api_key=api_keys['composio_key'])\n",
    "        self.tool_shemas={\n",
    "            'Notion Manager':{tool.name:tool for tool in self.tools.get_action_schemas(apps=[App.NOTION])}}\n",
    "        self.memory=Message_state(messages=[])\n",
    "        self.llm=OpenAIModel('gpt-4.1-nano',provider=OpenAIProvider(api_key=api_keys['openai_api_key']))\n",
    "        self.mcp_server=MCPServerStreamableHTTP(self.mcp_server_url)\n",
    "        self.agent=Agent(self.llm, mcp_servers=[self.mcp_server])\n",
    "\n",
    "    async def main(self,query:str):\n",
    "        async with self.agent.run_mcp_servers():  \n",
    "            result = self.agent.run_sync(query, message_history=self.memory.messages)\n",
    "            self.memory.messages=result.all_messages()\n",
    "        return result.output\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.messages=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
